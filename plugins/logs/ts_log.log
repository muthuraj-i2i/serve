2024-02-21T22:34:49,939 [WARN ] main org.pytorch.serve.util.ConfigManager - Your torchserve instance can access any URL to load models. When deploying to production, make sure to limit the set of allowed_urls in config.properties
2024-02-21T22:34:49,939 [WARN ] main org.pytorch.serve.util.ConfigManager - Your torchserve instance can access any URL to load models. When deploying to production, make sure to limit the set of allowed_urls in config.properties
2024-02-21T22:34:49,942 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...
2024-02-21T22:34:49,942 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...
2024-02-21T22:34:50,009 [INFO ] main org.pytorch.serve.metrics.configuration.MetricConfiguration - Successfully loaded metrics configuration from /opt/conda/lib/python3.10/site-packages/ts/configs/metrics.yaml
2024-02-21T22:34:50,009 [INFO ] main org.pytorch.serve.metrics.configuration.MetricConfiguration - Successfully loaded metrics configuration from /opt/conda/lib/python3.10/site-packages/ts/configs/metrics.yaml
2024-02-21T22:34:50,104 [INFO ] main org.pytorch.serve.ModelServer -
Torchserve version: 0.9.0
TS Home: /opt/conda/lib/python3.10/site-packages
Current directory: /home/ubuntu/serve/plugins
Temp directory: /tmp
Metrics config path: /opt/conda/lib/python3.10/site-packages/ts/configs/metrics.yaml
Number of GPUs: 1
Number of CPUs: 4
Max heap size: 3924 M
Python executable: /opt/conda/bin/python
Config file: N/A
Inference address: http://127.0.0.1:8080
Management address: http://127.0.0.1:8081
Metrics address: http://127.0.0.1:8082
Model Store: /home/ubuntu/serve/model_store
Initial Models: densenet161.mar
Log dir: /home/ubuntu/serve/plugins/logs
Metrics dir: /home/ubuntu/serve/plugins/logs
Netty threads: 0
Netty client threads: 0
Default workers per model: 1
Blacklist Regex: N/A
Maximum Response Size: 6553500
Maximum Request Size: 6553500
Limit Maximum Image Pixels: true
Prefer direct buffer: false
Allowed Urls: [file://.*|http(s)?://.*]
Custom python dependency for model allowed: false
Enable metrics API: true
Metrics mode: LOG
Disable system metrics: false
Workflow Store: /home/ubuntu/serve/model_store
CPP log config: N/A
Model config: N/A
2024-02-21T22:34:50,104 [INFO ] main org.pytorch.serve.ModelServer -
Torchserve version: 0.9.0
TS Home: /opt/conda/lib/python3.10/site-packages
Current directory: /home/ubuntu/serve/plugins
Temp directory: /tmp
Metrics config path: /opt/conda/lib/python3.10/site-packages/ts/configs/metrics.yaml
Number of GPUs: 1
Number of CPUs: 4
Max heap size: 3924 M
Python executable: /opt/conda/bin/python
Config file: N/A
Inference address: http://127.0.0.1:8080
Management address: http://127.0.0.1:8081
Metrics address: http://127.0.0.1:8082
Model Store: /home/ubuntu/serve/model_store
Initial Models: densenet161.mar
Log dir: /home/ubuntu/serve/plugins/logs
Metrics dir: /home/ubuntu/serve/plugins/logs
Netty threads: 0
Netty client threads: 0
Default workers per model: 1
Blacklist Regex: N/A
Maximum Response Size: 6553500
Maximum Request Size: 6553500
Limit Maximum Image Pixels: true
Prefer direct buffer: false
Allowed Urls: [file://.*|http(s)?://.*]
Custom python dependency for model allowed: false
Enable metrics API: true
Metrics mode: LOG
Disable system metrics: false
Workflow Store: /home/ubuntu/serve/model_store
CPP log config: N/A
Model config: N/A
2024-02-21T22:34:50,112 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...
2024-02-21T22:34:50,112 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...
2024-02-21T22:34:50,133 [INFO ] main org.pytorch.serve.ModelServer - Loading initial models: densenet161.mar
2024-02-21T22:34:50,133 [INFO ] main org.pytorch.serve.ModelServer - Loading initial models: densenet161.mar
2024-02-21T22:34:51,942 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model densenet161
2024-02-21T22:34:51,942 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model densenet161
2024-02-21T22:34:51,942 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model densenet161
2024-02-21T22:34:51,942 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model densenet161
2024-02-21T22:34:51,942 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model densenet161 loaded.
2024-02-21T22:34:51,942 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model densenet161 loaded.
2024-02-21T22:34:51,943 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: densenet161, count: 1
2024-02-21T22:34:51,943 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: densenet161, count: 1
2024-02-21T22:34:51,954 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: EpollServerSocketChannel.
2024-02-21T22:34:51,954 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: EpollServerSocketChannel.
2024-02-21T22:34:51,954 [DEBUG] W-9000-densenet161_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/opt/conda/bin/python, /opt/conda/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /opt/conda/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2024-02-21T22:34:51,954 [DEBUG] W-9000-densenet161_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/opt/conda/bin/python, /opt/conda/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /opt/conda/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2024-02-21T22:34:52,079 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://127.0.0.1:8080
2024-02-21T22:34:52,079 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://127.0.0.1:8080
2024-02-21T22:34:52,080 [INFO ] main org.pytorch.serve.ModelServer - Initialize Management server with: EpollServerSocketChannel.
2024-02-21T22:34:52,080 [INFO ] main org.pytorch.serve.ModelServer - Initialize Management server with: EpollServerSocketChannel.
2024-02-21T22:34:52,083 [INFO ] main org.pytorch.serve.ModelServer - Management API bind to: http://127.0.0.1:8081
2024-02-21T22:34:52,083 [INFO ] main org.pytorch.serve.ModelServer - Management API bind to: http://127.0.0.1:8081
2024-02-21T22:34:52,090 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: EpollServerSocketChannel.
2024-02-21T22:34:52,090 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: EpollServerSocketChannel.
2024-02-21T22:34:52,093 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://127.0.0.1:8082
2024-02-21T22:34:52,093 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://127.0.0.1:8082
2024-02-21T22:34:52,380 [WARN ] pool-3-thread-1 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.
2024-02-21T22:34:52,380 [WARN ] pool-3-thread-1 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.
2024-02-21T22:34:52,975 [INFO ] pool-3-thread-1 TS_METRICS - CPUUtilization.Percent:0.0|#Level:Host|#hostname:ip-172-31-11-32,timestamp:1708554892
2024-02-21T22:34:52,977 [INFO ] pool-3-thread-1 TS_METRICS - DiskAvailable.Gigabytes:343.5322265625|#Level:Host|#hostname:ip-172-31-11-32,timestamp:1708554892
2024-02-21T22:34:52,978 [INFO ] pool-3-thread-1 TS_METRICS - DiskUsage.Gigabytes:44.13956069946289|#Level:Host|#hostname:ip-172-31-11-32,timestamp:1708554892
2024-02-21T22:34:52,979 [INFO ] pool-3-thread-1 TS_METRICS - DiskUtilization.Percent:11.4|#Level:Host|#hostname:ip-172-31-11-32,timestamp:1708554892
2024-02-21T22:34:52,979 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUtilization.Percent:0.0|#Level:Host,DeviceId:0|#hostname:ip-172-31-11-32,timestamp:1708554892
2024-02-21T22:34:52,980 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUsed.Megabytes:0.0|#Level:Host,DeviceId:0|#hostname:ip-172-31-11-32,timestamp:1708554892
2024-02-21T22:34:52,980 [INFO ] pool-3-thread-1 TS_METRICS - GPUUtilization.Percent:0.0|#Level:Host,DeviceId:0|#hostname:ip-172-31-11-32,timestamp:1708554892
2024-02-21T22:34:52,981 [INFO ] pool-3-thread-1 TS_METRICS - MemoryAvailable.Megabytes:13363.5078125|#Level:Host|#hostname:ip-172-31-11-32,timestamp:1708554892
2024-02-21T22:34:52,981 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUsed.Megabytes:2002.59765625|#Level:Host|#hostname:ip-172-31-11-32,timestamp:1708554892
2024-02-21T22:34:52,982 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUtilization.Percent:14.8|#Level:Host|#hostname:ip-172-31-11-32,timestamp:1708554892
2024-02-21T22:34:53,578 [INFO ] W-9000-densenet161_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9000, pid=44164
2024-02-21T22:34:53,579 [INFO ] W-9000-densenet161_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2024-02-21T22:34:53,589 [INFO ] W-9000-densenet161_1.0-stdout MODEL_LOG - Successfully loaded /opt/conda/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2024-02-21T22:34:53,589 [INFO ] W-9000-densenet161_1.0-stdout MODEL_LOG - [PID]44164
2024-02-21T22:34:53,589 [INFO ] W-9000-densenet161_1.0-stdout MODEL_LOG - Torch worker started.
2024-02-21T22:34:53,590 [INFO ] W-9000-densenet161_1.0-stdout MODEL_LOG - Python runtime: 3.10.9
2024-02-21T22:34:53,590 [DEBUG] W-9000-densenet161_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-densenet161_1.0 State change null -> WORKER_STARTED
2024-02-21T22:34:53,590 [DEBUG] W-9000-densenet161_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-densenet161_1.0 State change null -> WORKER_STARTED
2024-02-21T22:34:53,594 [INFO ] W-9000-densenet161_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2024-02-21T22:34:53,594 [INFO ] W-9000-densenet161_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2024-02-21T22:34:53,602 [INFO ] W-9000-densenet161_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2024-02-21T22:34:53,605 [DEBUG] W-9000-densenet161_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1708554893605
2024-02-21T22:34:53,605 [DEBUG] W-9000-densenet161_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1708554893605
2024-02-21T22:34:53,607 [INFO ] W-9000-densenet161_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1708554893607
2024-02-21T22:34:53,607 [INFO ] W-9000-densenet161_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1708554893607
2024-02-21T22:34:53,640 [INFO ] W-9000-densenet161_1.0-stdout MODEL_LOG - model_name: densenet161, batchSize: 1
2024-02-21T22:34:54,578 [INFO ] W-9000-densenet161_1.0-stdout MODEL_LOG - ONNX enabled
2024-02-21T22:34:54,579 [INFO ] W-9000-densenet161_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2024-02-21T22:34:55,202 [INFO ] W-9000-densenet161_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 1595
2024-02-21T22:34:55,202 [INFO ] W-9000-densenet161_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 1595
2024-02-21T22:34:55,203 [DEBUG] W-9000-densenet161_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-densenet161_1.0 State change WORKER_STARTED -> WORKER_MODEL_LOADED
2024-02-21T22:34:55,203 [DEBUG] W-9000-densenet161_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-densenet161_1.0 State change WORKER_STARTED -> WORKER_MODEL_LOADED
2024-02-21T22:34:55,203 [INFO ] W-9000-densenet161_1.0 TS_METRICS - WorkerLoadTime.Milliseconds:3255.0|#WorkerName:W-9000-densenet161_1.0,Level:Host|#hostname:ip-172-31-11-32,timestamp:1708554895
2024-02-21T22:34:55,204 [INFO ] W-9000-densenet161_1.0 TS_METRICS - WorkerThreadTime.Milliseconds:4.0|#Level:Host|#hostname:ip-172-31-11-32,timestamp:1708554895
2024-02-21T22:35:10,655 [WARN ] main org.pytorch.serve.util.ConfigManager - Your torchserve instance can access any URL to load models. When deploying to production, make sure to limit the set of allowed_urls in config.properties
2024-02-21T22:35:10,655 [WARN ] main org.pytorch.serve.util.ConfigManager - Your torchserve instance can access any URL to load models. When deploying to production, make sure to limit the set of allowed_urls in config.properties
2024-02-21T22:35:10,658 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...
2024-02-21T22:35:10,658 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...
2024-02-21T22:35:10,715 [INFO ] main org.pytorch.serve.metrics.configuration.MetricConfiguration - Successfully loaded metrics configuration from /opt/conda/lib/python3.10/site-packages/ts/configs/metrics.yaml
2024-02-21T22:35:10,715 [INFO ] main org.pytorch.serve.metrics.configuration.MetricConfiguration - Successfully loaded metrics configuration from /opt/conda/lib/python3.10/site-packages/ts/configs/metrics.yaml
2024-02-21T22:35:10,811 [INFO ] main org.pytorch.serve.ModelServer -
Torchserve version: 0.9.0
TS Home: /opt/conda/lib/python3.10/site-packages
Current directory: /home/ubuntu/serve/plugins
Temp directory: /tmp
Metrics config path: /opt/conda/lib/python3.10/site-packages/ts/configs/metrics.yaml
Number of GPUs: 1
Number of CPUs: 4
Max heap size: 3924 M
Python executable: /opt/conda/bin/python
Config file: N/A
Inference address: http://127.0.0.1:8080
Management address: http://127.0.0.1:8081
Metrics address: http://127.0.0.1:8082
Model Store: /home/ubuntu/serve/model_store
Initial Models: densenet161.mar
Log dir: /home/ubuntu/serve/plugins/logs
Metrics dir: /home/ubuntu/serve/plugins/logs
Netty threads: 0
Netty client threads: 0
Default workers per model: 1
Blacklist Regex: N/A
Maximum Response Size: 6553500
Maximum Request Size: 6553500
Limit Maximum Image Pixels: true
Prefer direct buffer: false
Allowed Urls: [file://.*|http(s)?://.*]
Custom python dependency for model allowed: false
Enable metrics API: true
Metrics mode: LOG
Disable system metrics: false
Workflow Store: /home/ubuntu/serve/model_store
CPP log config: N/A
Model config: N/A
2024-02-21T22:35:10,811 [INFO ] main org.pytorch.serve.ModelServer -
Torchserve version: 0.9.0
TS Home: /opt/conda/lib/python3.10/site-packages
Current directory: /home/ubuntu/serve/plugins
Temp directory: /tmp
Metrics config path: /opt/conda/lib/python3.10/site-packages/ts/configs/metrics.yaml
Number of GPUs: 1
Number of CPUs: 4
Max heap size: 3924 M
Python executable: /opt/conda/bin/python
Config file: N/A
Inference address: http://127.0.0.1:8080
Management address: http://127.0.0.1:8081
Metrics address: http://127.0.0.1:8082
Model Store: /home/ubuntu/serve/model_store
Initial Models: densenet161.mar
Log dir: /home/ubuntu/serve/plugins/logs
Metrics dir: /home/ubuntu/serve/plugins/logs
Netty threads: 0
Netty client threads: 0
Default workers per model: 1
Blacklist Regex: N/A
Maximum Response Size: 6553500
Maximum Request Size: 6553500
Limit Maximum Image Pixels: true
Prefer direct buffer: false
Allowed Urls: [file://.*|http(s)?://.*]
Custom python dependency for model allowed: false
Enable metrics API: true
Metrics mode: LOG
Disable system metrics: false
Workflow Store: /home/ubuntu/serve/model_store
CPP log config: N/A
Model config: N/A
2024-02-21T22:35:10,820 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...
2024-02-21T22:35:10,820 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...
2024-02-21T22:35:10,841 [INFO ] main org.pytorch.serve.ModelServer - Loading initial models: densenet161.mar
2024-02-21T22:35:10,841 [INFO ] main org.pytorch.serve.ModelServer - Loading initial models: densenet161.mar
2024-02-21T22:35:12,631 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model densenet161
2024-02-21T22:35:12,631 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model densenet161
2024-02-21T22:35:12,631 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model densenet161
2024-02-21T22:35:12,631 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model densenet161
2024-02-21T22:35:12,632 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model densenet161 loaded.
2024-02-21T22:35:12,632 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model densenet161 loaded.
2024-02-21T22:35:12,632 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: densenet161, count: 1
2024-02-21T22:35:12,632 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: densenet161, count: 1
2024-02-21T22:35:12,644 [DEBUG] W-9000-densenet161_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/opt/conda/bin/python, /opt/conda/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /opt/conda/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2024-02-21T22:35:12,644 [DEBUG] W-9000-densenet161_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/opt/conda/bin/python, /opt/conda/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /opt/conda/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2024-02-21T22:35:12,645 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: EpollServerSocketChannel.
2024-02-21T22:35:12,645 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: EpollServerSocketChannel.
2024-02-21T22:35:12,729 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://127.0.0.1:8080
2024-02-21T22:35:12,729 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://127.0.0.1:8080
2024-02-21T22:35:12,729 [INFO ] main org.pytorch.serve.ModelServer - Initialize Management server with: EpollServerSocketChannel.
2024-02-21T22:35:12,729 [INFO ] main org.pytorch.serve.ModelServer - Initialize Management server with: EpollServerSocketChannel.
2024-02-21T22:35:12,730 [INFO ] main org.pytorch.serve.ModelServer - Management API bind to: http://127.0.0.1:8081
2024-02-21T22:35:12,730 [INFO ] main org.pytorch.serve.ModelServer - Management API bind to: http://127.0.0.1:8081
2024-02-21T22:35:12,731 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: EpollServerSocketChannel.
2024-02-21T22:35:12,731 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: EpollServerSocketChannel.
2024-02-21T22:35:12,733 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://127.0.0.1:8082
2024-02-21T22:35:12,733 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://127.0.0.1:8082
2024-02-21T22:35:13,042 [WARN ] pool-3-thread-1 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.
2024-02-21T22:35:13,042 [WARN ] pool-3-thread-1 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.
2024-02-21T22:35:13,765 [INFO ] pool-3-thread-1 TS_METRICS - CPUUtilization.Percent:100.0|#Level:Host|#hostname:ip-172-31-11-32,timestamp:1708554913
2024-02-21T22:35:13,766 [INFO ] pool-3-thread-1 TS_METRICS - DiskAvailable.Gigabytes:343.5321960449219|#Level:Host|#hostname:ip-172-31-11-32,timestamp:1708554913
2024-02-21T22:35:13,767 [INFO ] pool-3-thread-1 TS_METRICS - DiskUsage.Gigabytes:44.139591217041016|#Level:Host|#hostname:ip-172-31-11-32,timestamp:1708554913
2024-02-21T22:35:13,768 [INFO ] pool-3-thread-1 TS_METRICS - DiskUtilization.Percent:11.4|#Level:Host|#hostname:ip-172-31-11-32,timestamp:1708554913
2024-02-21T22:35:13,768 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUtilization.Percent:0.0|#Level:Host,DeviceId:0|#hostname:ip-172-31-11-32,timestamp:1708554913
2024-02-21T22:35:13,769 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUsed.Megabytes:0.0|#Level:Host,DeviceId:0|#hostname:ip-172-31-11-32,timestamp:1708554913
2024-02-21T22:35:13,769 [INFO ] pool-3-thread-1 TS_METRICS - GPUUtilization.Percent:0.0|#Level:Host,DeviceId:0|#hostname:ip-172-31-11-32,timestamp:1708554913
2024-02-21T22:35:13,770 [INFO ] pool-3-thread-1 TS_METRICS - MemoryAvailable.Megabytes:13359.02734375|#Level:Host|#hostname:ip-172-31-11-32,timestamp:1708554913
2024-02-21T22:35:13,771 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUsed.Megabytes:2007.0546875|#Level:Host|#hostname:ip-172-31-11-32,timestamp:1708554913
2024-02-21T22:35:13,772 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUtilization.Percent:14.9|#Level:Host|#hostname:ip-172-31-11-32,timestamp:1708554913
2024-02-21T22:35:14,412 [INFO ] W-9000-densenet161_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9000, pid=44363
2024-02-21T22:35:14,413 [INFO ] W-9000-densenet161_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2024-02-21T22:35:14,422 [INFO ] W-9000-densenet161_1.0-stdout MODEL_LOG - Successfully loaded /opt/conda/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2024-02-21T22:35:14,423 [INFO ] W-9000-densenet161_1.0-stdout MODEL_LOG - [PID]44363
2024-02-21T22:35:14,423 [INFO ] W-9000-densenet161_1.0-stdout MODEL_LOG - Torch worker started.
2024-02-21T22:35:14,423 [INFO ] W-9000-densenet161_1.0-stdout MODEL_LOG - Python runtime: 3.10.9
2024-02-21T22:35:14,424 [DEBUG] W-9000-densenet161_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-densenet161_1.0 State change null -> WORKER_STARTED
2024-02-21T22:35:14,424 [DEBUG] W-9000-densenet161_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-densenet161_1.0 State change null -> WORKER_STARTED
2024-02-21T22:35:14,428 [INFO ] W-9000-densenet161_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2024-02-21T22:35:14,428 [INFO ] W-9000-densenet161_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2024-02-21T22:35:14,435 [INFO ] W-9000-densenet161_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2024-02-21T22:35:14,437 [DEBUG] W-9000-densenet161_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1708554914437
2024-02-21T22:35:14,437 [DEBUG] W-9000-densenet161_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1708554914437
2024-02-21T22:35:14,441 [INFO ] W-9000-densenet161_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1708554914441
2024-02-21T22:35:14,441 [INFO ] W-9000-densenet161_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1708554914441
2024-02-21T22:35:14,472 [INFO ] W-9000-densenet161_1.0-stdout MODEL_LOG - model_name: densenet161, batchSize: 1
2024-02-21T22:35:15,445 [INFO ] W-9000-densenet161_1.0-stdout MODEL_LOG - ONNX enabled
2024-02-21T22:35:15,446 [INFO ] W-9000-densenet161_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2024-02-21T22:35:16,074 [INFO ] W-9000-densenet161_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 1633
2024-02-21T22:35:16,074 [INFO ] W-9000-densenet161_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 1633
2024-02-21T22:35:16,074 [DEBUG] W-9000-densenet161_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-densenet161_1.0 State change WORKER_STARTED -> WORKER_MODEL_LOADED
2024-02-21T22:35:16,074 [DEBUG] W-9000-densenet161_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-densenet161_1.0 State change WORKER_STARTED -> WORKER_MODEL_LOADED
2024-02-21T22:35:16,075 [INFO ] W-9000-densenet161_1.0 TS_METRICS - WorkerLoadTime.Milliseconds:3435.0|#WorkerName:W-9000-densenet161_1.0,Level:Host|#hostname:ip-172-31-11-32,timestamp:1708554916
2024-02-21T22:35:16,075 [INFO ] W-9000-densenet161_1.0 TS_METRICS - WorkerThreadTime.Milliseconds:5.0|#Level:Host|#hostname:ip-172-31-11-32,timestamp:1708554916
2024-02-21T22:35:34,772 [INFO ] epollEventLoopGroup-3-1 ACCESS_LOG - /127.0.0.1:60022 "GET /models/densenet161 HTTP/1.1" 200 63
2024-02-21T22:35:34,773 [INFO ] epollEventLoopGroup-3-1 TS_METRICS - Requests2XX.Count:1.0|#Level:Host|#hostname:ip-172-31-11-32,timestamp:1708554934
2024-02-21T22:35:51,463 [WARN ] main org.pytorch.serve.util.ConfigManager - Your torchserve instance can access any URL to load models. When deploying to production, make sure to limit the set of allowed_urls in config.properties
2024-02-21T22:35:51,463 [WARN ] main org.pytorch.serve.util.ConfigManager - Your torchserve instance can access any URL to load models. When deploying to production, make sure to limit the set of allowed_urls in config.properties
2024-02-21T22:35:51,465 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...
2024-02-21T22:35:51,465 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...
2024-02-21T22:35:51,530 [INFO ] main org.pytorch.serve.metrics.configuration.MetricConfiguration - Successfully loaded metrics configuration from /opt/conda/lib/python3.10/site-packages/ts/configs/metrics.yaml
2024-02-21T22:35:51,530 [INFO ] main org.pytorch.serve.metrics.configuration.MetricConfiguration - Successfully loaded metrics configuration from /opt/conda/lib/python3.10/site-packages/ts/configs/metrics.yaml
2024-02-21T22:35:51,625 [INFO ] main org.pytorch.serve.ModelServer -
Torchserve version: 0.9.0
TS Home: /opt/conda/lib/python3.10/site-packages
Current directory: /home/ubuntu/serve/plugins
Temp directory: /tmp
Metrics config path: /opt/conda/lib/python3.10/site-packages/ts/configs/metrics.yaml
Number of GPUs: 1
Number of CPUs: 4
Max heap size: 3924 M
Python executable: /opt/conda/bin/python
Config file: N/A
Inference address: http://127.0.0.1:8080
Management address: http://127.0.0.1:8081
Metrics address: http://127.0.0.1:8082
Model Store: /home/ubuntu/serve/model_store
Initial Models: densenet161.mar
Log dir: /home/ubuntu/serve/plugins/logs
Metrics dir: /home/ubuntu/serve/plugins/logs
Netty threads: 0
Netty client threads: 0
Default workers per model: 1
Blacklist Regex: N/A
Maximum Response Size: 6553500
Maximum Request Size: 6553500
Limit Maximum Image Pixels: true
Prefer direct buffer: false
Allowed Urls: [file://.*|http(s)?://.*]
Custom python dependency for model allowed: false
Enable metrics API: true
Metrics mode: LOG
Disable system metrics: false
Workflow Store: /home/ubuntu/serve/model_store
CPP log config: N/A
Model config: N/A
2024-02-21T22:35:51,625 [INFO ] main org.pytorch.serve.ModelServer -
Torchserve version: 0.9.0
TS Home: /opt/conda/lib/python3.10/site-packages
Current directory: /home/ubuntu/serve/plugins
Temp directory: /tmp
Metrics config path: /opt/conda/lib/python3.10/site-packages/ts/configs/metrics.yaml
Number of GPUs: 1
Number of CPUs: 4
Max heap size: 3924 M
Python executable: /opt/conda/bin/python
Config file: N/A
Inference address: http://127.0.0.1:8080
Management address: http://127.0.0.1:8081
Metrics address: http://127.0.0.1:8082
Model Store: /home/ubuntu/serve/model_store
Initial Models: densenet161.mar
Log dir: /home/ubuntu/serve/plugins/logs
Metrics dir: /home/ubuntu/serve/plugins/logs
Netty threads: 0
Netty client threads: 0
Default workers per model: 1
Blacklist Regex: N/A
Maximum Response Size: 6553500
Maximum Request Size: 6553500
Limit Maximum Image Pixels: true
Prefer direct buffer: false
Allowed Urls: [file://.*|http(s)?://.*]
Custom python dependency for model allowed: false
Enable metrics API: true
Metrics mode: LOG
Disable system metrics: false
Workflow Store: /home/ubuntu/serve/model_store
CPP log config: N/A
Model config: N/A
2024-02-21T22:35:51,634 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...
2024-02-21T22:35:51,634 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...
2024-02-21T22:35:51,658 [INFO ] main org.pytorch.serve.ModelServer - Loading initial models: densenet161.mar
2024-02-21T22:35:51,658 [INFO ] main org.pytorch.serve.ModelServer - Loading initial models: densenet161.mar
2024-02-21T22:35:53,492 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model densenet161
2024-02-21T22:35:53,492 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model densenet161
2024-02-21T22:35:53,492 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model densenet161
2024-02-21T22:35:53,492 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model densenet161
2024-02-21T22:35:53,492 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model densenet161 loaded.
2024-02-21T22:35:53,492 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model densenet161 loaded.
2024-02-21T22:35:53,493 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: densenet161, count: 1
2024-02-21T22:35:53,493 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: densenet161, count: 1
2024-02-21T22:35:53,506 [DEBUG] W-9000-densenet161_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/opt/conda/bin/python, /opt/conda/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /opt/conda/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2024-02-21T22:35:53,508 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: EpollServerSocketChannel.
2024-02-21T22:35:53,508 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: EpollServerSocketChannel.
2024-02-21T22:35:53,506 [DEBUG] W-9000-densenet161_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/opt/conda/bin/python, /opt/conda/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /opt/conda/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2024-02-21T22:35:53,586 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://127.0.0.1:8080
2024-02-21T22:35:53,586 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://127.0.0.1:8080
2024-02-21T22:35:53,587 [INFO ] main org.pytorch.serve.ModelServer - Initialize Management server with: EpollServerSocketChannel.
2024-02-21T22:35:53,587 [INFO ] main org.pytorch.serve.ModelServer - Initialize Management server with: EpollServerSocketChannel.
2024-02-21T22:35:53,592 [INFO ] main org.pytorch.serve.ModelServer - Management API bind to: http://127.0.0.1:8081
2024-02-21T22:35:53,592 [INFO ] main org.pytorch.serve.ModelServer - Management API bind to: http://127.0.0.1:8081
2024-02-21T22:35:53,592 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: EpollServerSocketChannel.
2024-02-21T22:35:53,592 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: EpollServerSocketChannel.
2024-02-21T22:35:53,596 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://127.0.0.1:8082
2024-02-21T22:35:53,596 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://127.0.0.1:8082
2024-02-21T22:35:53,876 [WARN ] pool-3-thread-1 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.
2024-02-21T22:35:53,876 [WARN ] pool-3-thread-1 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.
2024-02-21T22:35:54,435 [INFO ] pool-3-thread-1 TS_METRICS - CPUUtilization.Percent:66.7|#Level:Host|#hostname:ip-172-31-11-32,timestamp:1708554954
2024-02-21T22:35:54,437 [INFO ] pool-3-thread-1 TS_METRICS - DiskAvailable.Gigabytes:343.53216552734375|#Level:Host|#hostname:ip-172-31-11-32,timestamp:1708554954
2024-02-21T22:35:54,438 [INFO ] pool-3-thread-1 TS_METRICS - DiskUsage.Gigabytes:44.13962173461914|#Level:Host|#hostname:ip-172-31-11-32,timestamp:1708554954
2024-02-21T22:35:54,438 [INFO ] pool-3-thread-1 TS_METRICS - DiskUtilization.Percent:11.4|#Level:Host|#hostname:ip-172-31-11-32,timestamp:1708554954
2024-02-21T22:35:54,440 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUtilization.Percent:0.0|#Level:Host,DeviceId:0|#hostname:ip-172-31-11-32,timestamp:1708554954
2024-02-21T22:35:54,441 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUsed.Megabytes:0.0|#Level:Host,DeviceId:0|#hostname:ip-172-31-11-32,timestamp:1708554954
2024-02-21T22:35:54,442 [INFO ] pool-3-thread-1 TS_METRICS - GPUUtilization.Percent:0.0|#Level:Host,DeviceId:0|#hostname:ip-172-31-11-32,timestamp:1708554954
2024-02-21T22:35:54,442 [INFO ] pool-3-thread-1 TS_METRICS - MemoryAvailable.Megabytes:13387.25|#Level:Host|#hostname:ip-172-31-11-32,timestamp:1708554954
2024-02-21T22:35:54,442 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUsed.Megabytes:1978.91015625|#Level:Host|#hostname:ip-172-31-11-32,timestamp:1708554954
2024-02-21T22:35:54,443 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUtilization.Percent:14.7|#Level:Host|#hostname:ip-172-31-11-32,timestamp:1708554954
2024-02-21T22:35:55,064 [INFO ] W-9000-densenet161_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9000, pid=44644
2024-02-21T22:35:55,065 [INFO ] W-9000-densenet161_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2024-02-21T22:35:55,073 [INFO ] W-9000-densenet161_1.0-stdout MODEL_LOG - Successfully loaded /opt/conda/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2024-02-21T22:35:55,073 [INFO ] W-9000-densenet161_1.0-stdout MODEL_LOG - [PID]44644
2024-02-21T22:35:55,074 [INFO ] W-9000-densenet161_1.0-stdout MODEL_LOG - Torch worker started.
2024-02-21T22:35:55,074 [INFO ] W-9000-densenet161_1.0-stdout MODEL_LOG - Python runtime: 3.10.9
2024-02-21T22:35:55,074 [DEBUG] W-9000-densenet161_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-densenet161_1.0 State change null -> WORKER_STARTED
2024-02-21T22:35:55,074 [DEBUG] W-9000-densenet161_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-densenet161_1.0 State change null -> WORKER_STARTED
2024-02-21T22:35:55,087 [INFO ] W-9000-densenet161_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2024-02-21T22:35:55,087 [INFO ] W-9000-densenet161_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2024-02-21T22:35:55,094 [INFO ] W-9000-densenet161_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2024-02-21T22:35:55,097 [DEBUG] W-9000-densenet161_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1708554955097
2024-02-21T22:35:55,097 [DEBUG] W-9000-densenet161_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1708554955097
2024-02-21T22:35:55,100 [INFO ] W-9000-densenet161_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1708554955100
2024-02-21T22:35:55,100 [INFO ] W-9000-densenet161_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1708554955100
2024-02-21T22:35:55,133 [INFO ] W-9000-densenet161_1.0-stdout MODEL_LOG - model_name: densenet161, batchSize: 1
2024-02-21T22:35:56,105 [INFO ] W-9000-densenet161_1.0-stdout MODEL_LOG - ONNX enabled
2024-02-21T22:35:56,105 [INFO ] W-9000-densenet161_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2024-02-21T22:35:56,731 [INFO ] W-9000-densenet161_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 1631
2024-02-21T22:35:56,731 [INFO ] W-9000-densenet161_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 1631
2024-02-21T22:35:56,732 [DEBUG] W-9000-densenet161_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-densenet161_1.0 State change WORKER_STARTED -> WORKER_MODEL_LOADED
2024-02-21T22:35:56,732 [DEBUG] W-9000-densenet161_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-densenet161_1.0 State change WORKER_STARTED -> WORKER_MODEL_LOADED
2024-02-21T22:35:56,732 [INFO ] W-9000-densenet161_1.0 TS_METRICS - WorkerLoadTime.Milliseconds:3232.0|#WorkerName:W-9000-densenet161_1.0,Level:Host|#hostname:ip-172-31-11-32,timestamp:1708554956
2024-02-21T22:35:56,733 [INFO ] W-9000-densenet161_1.0 TS_METRICS - WorkerThreadTime.Milliseconds:5.0|#Level:Host|#hostname:ip-172-31-11-32,timestamp:1708554956
